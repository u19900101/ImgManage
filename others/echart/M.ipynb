{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.从html提取信息写入csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 调用外部的py\r\n",
    "from demo import *\r\n",
    "hello()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.将空缺的日期补齐"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from datetime import datetime\r\n",
    "# First：找到没有进行创作的日子\r\n",
    "# 1.读取文件 第一行为列名称\r\n",
    "df=pd.read_csv('../csv绘图/info.csv',header=0,sep=',') #filename可以直接从盘符开始，标明每一级的文件夹直到csv文件，header=None表示头部为空，sep=' '表示数据间使用空格作为分隔符，如果分隔符是逗号，只需换成 ‘，’即可。\r\n",
    "# 2.按日期进行升序排列\r\n",
    "df2 = df.sort_values(by='day')\r\n",
    "# 3.提取出 day列  将其转化为 yyyy/mm/dd的格式  将 str=2017/1/27 23:06  转化为  datetime类型\r\n",
    "d1 = [datetime.strptime(x, '%Y/%m/%d %H:%M') for x in list(df2[\"day\"])]\r\n",
    "#       将 datetime 转化为 str 2017/01/27\r\n",
    "d_2017 = [datetime.strftime(x, '%Y/%m/%d') for x in list(d1)]\r\n",
    "# 4.创建全年的日子\r\n",
    "all_2017=[datetime.strftime(x,'%Y/%m/%d') for x in list(pd.date_range(start='2017/01/01',  end=\"2017/12/31\"))]\r\n",
    "# 5.将上两项做差求出没有记录的日子\r\n",
    "absent_2017 = [i for i in all_2017 if i not in d_2017]\r\n",
    "\r\n",
    "# 6.将没有记录的日子进行补充  [day:当天,title:空,longitude,latitude,num:0]\r\n",
    "dataframe = pd.DataFrame({'day': absent_2017,\r\n",
    "                            'title': [None]*len(absent_2017),\r\n",
    "                            'longitude': [None]*len(absent_2017),\r\n",
    "                            'latitude': [None]*len(absent_2017),\r\n",
    "                            'num': [0]*len(absent_2017)})\r\n",
    "dataframe.to_csv(\"./info.csv\",index=False, sep=',',mode='a', header=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "absent_2017 = ['2017/07/10','2017/07/12','2017/07/11']\r\n",
    "absent_2017_np = [time.mktime(time.strptime(x, '%Y/%m/%d')) for x in absent_2017]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\r\n",
    "import time       \r\n",
    "timeArray = time.strptime('2017/7/11 00:00', '%Y/%m/%d %H:%M')\r\n",
    "#转换成时间戳\r\n",
    "time.mktime(timeArray)     "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "absent_2017_np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.处理重复的日子"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 手动修改日期 并检查查看"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "# First：找到没有进行创作的日子\r\n",
    "# 1.读取文件 第一行为列名称\r\n",
    "res_path = './data/我的抗战2.0_2020_info.csv'\r\n",
    "# res_path = './data/2020_info.csv'\r\n",
    "df=pd.read_csv(res_path,header=0,sep=',') \r\n",
    "# 手动修改 日期  \r\n",
    "# 截取day的date————2017/02/02  是否唯一进行检查\r\n",
    "M = [x.split(\" \")[0] for x in df[\"day\"].tolist()]\r\n",
    "# 给原 df加一列 date\r\n",
    "df['day_date'] = M\r\n",
    "# 得到date重复的行  保留出现第一次的值  duplicated(keep=False)\r\n",
    "day_date = df[df['day_date'].duplicated(keep=False)]\r\n",
    "M = day_date[['day','title','day_date']]\r\n",
    "print(len(M))\r\n",
    "M"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2自动整理日期\r\n",
    "2019/08/19 12:00,,,,0,1566187200.0\r\n",
    "2019/08/20 15:02,3.196  转文全部被拒  心情失落  又开始考虑复原,0,0,28,1566284520.0\r\n",
    "2019/08/20 15:04,3.197  第一天便无法做到自律 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def edit_info(res_path):\r\n",
    "    # 手动修改 日期  \r\n",
    "    # 截取day的date————2017/02/02 12:26  -->  2017/02/02  是否唯一进行检查\r\n",
    "    df=pd.read_csv(res_path,header=0,sep=',') \r\n",
    "    M = [x.split(\" \")[0] for x in df[\"day\"].tolist()]\r\n",
    "    # 给原 df加一列 date\r\n",
    "    df['day_date'] = M\r\n",
    "    # 得到date重复的行  保留出现第一次的值  duplicated(keep=False)\r\n",
    "    day_date = df[df['day_date'].duplicated(keep=False)]\r\n",
    "    M = day_date[['day','title','day_date']]\r\n",
    "    print(\"重复的天数为：\",len(M))\r\n",
    "   \r\n",
    "    all_index = M.index.tolist() \r\n",
    "      # 剔除 0 元素\r\n",
    "    if 0 in all_index:\r\n",
    "        all_index.remove(0)\r\n",
    "    # 2.判断 index-1 是否为系统自动生成的 (num值是否为0)\r\n",
    "    del_index = []\r\n",
    "\r\n",
    "    for x in all_index:\r\n",
    "        if ((df[x:x+1].day_date == '2020/02/13').bool()):\r\n",
    "            print(\"2020/02/13\")\r\n",
    "        if ((df[x-1:x].num == 0).bool()):\r\n",
    "            print(\"删除行\",df[x-1:x].day)\r\n",
    "            del_index.append(x-1)\r\n",
    "            # df = df.drop([x-1])\r\n",
    "        # 3.是——删掉 index-1这一行 \r\n",
    "        # 4.修改 df[index] 的 date-1\r\n",
    "            temp = df[x:x+1]\r\n",
    "            dt = datetime.datetime.strptime(temp.at[temp.index.values[0],'day'],'%Y/%m/%d %H:%M')\r\n",
    "            out_date = (dt + datetime.timedelta(days=-1)).strftime('%Y/%m/%d %H:%M')\r\n",
    "            temp.day[x] = out_date\r\n",
    "    # 5.更新csv文件\r\n",
    "    df = df.drop(del_index)\r\n",
    "    df.to_csv(res_path,index=False, sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3将没有坐标的日子进行临近填充"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# info_path = 'D:\\MyJava\\mylifeImg\\others\\echart\\data\\demo.csv'\r\n",
    "# update_gps_info(info_path)\r\n",
    "dir = 'D:\\MyJava\\mylifeImg\\others\\echart\\data\\\\'\r\n",
    "update_gps_dir(dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 检验填充后的数据是否还有 gps 为 0 或 Nan\r\n",
    "import pandas as pd\r\n",
    "import time\r\n",
    "info_path = '../info.csv'\r\n",
    "m=pd.read_csv(info_path,header=0,sep=',') \r\n",
    "day_date = m[(m['longitude'].isna()) | (m['longitude']=='0')]['day_date']\r\n",
    "for x in day_date:\r\n",
    "    if time.strptime(x, '%Y/%m/%d')<time.localtime():\r\n",
    "        print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import requests\r\n",
    "import json\r\n",
    "import numpy as np\r\n",
    "# 比较耗时 注意保存文件\r\n",
    "# 添加一列  location_name  \r\n",
    "def get_locationName(lng,lat):\r\n",
    "    key = 'GjG3XAdmywz7CyETWqHwIuEC6ZExY6QT'\r\n",
    "    r = requests.get(url='http://api.map.baidu.com/geocoder/v2/', params={'location':str(lat)+','+str(lng),'ak':key,'output':'json'})\r\n",
    "    result = r.json()\r\n",
    "    # print(result)\r\n",
    "    province = result['result']['addressComponent']['province']\r\n",
    "    city = result['result']['addressComponent']['city']\r\n",
    "    district = result['result']['addressComponent']['district']\r\n",
    "    street = result['result']['addressComponent']['street']\r\n",
    "    street_number = result['result']['addressComponent']['street_number']\r\n",
    "    return city+\" \"+district+\" \"+street+\" \"+street_number\r\n",
    "    \r\n",
    "info_path = 'D:\\MyJava\\mylifeImg\\others\\info.csv'\r\n",
    "m=pd.read_csv(info_path,header=0,sep=',') \r\n",
    "\r\n",
    "location_list = []\r\n",
    "for index, row in m.iterrows():\r\n",
    "    if(np.isnan(row.longitude) or np.isnan(row.latitude)):\r\n",
    "        location_list.append('无位置信息')\r\n",
    "        continue\r\n",
    "    location_name = get_locationName(row.longitude,row.latitude)\r\n",
    "    location_list.append(location_name)\r\n",
    "m[\"location_name\"] = location_list\r\n",
    "m.to_csv(info_path,index=False, sep=',')\r\n",
    "# 单独写一个文件保存位置信息\r\n",
    "m[[\"day\",\"location_name\"]].to_csv('./data/location_name.csv',index=False, sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 调用外部的py\r\n",
    "from demo import *\r\n",
    "classifyByYear(info_path,info_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4将文件按年进行分表"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1.读取文件\r\n",
    "res_path = '../info.csv'\r\n",
    "df=pd.read_csv(res_path,header=0,sep=',') \r\n",
    "# 2.按照day_date(2018/01/01)  列的前四位  2018进行分表\r\n",
    "# day_date_list = list(df['day_date'])\r\n",
    "day_date_list = [x.split('/')[0] for x in list(df['day_date'])]\r\n",
    "year_list = list(set(day_date_list))\r\n",
    "year_list = [eval(x) for x in list(set(day_date_list))]\r\n",
    "# 3.将年份由小到大进行排序\r\n",
    "year_list.sort()\r\n",
    "# 4.分割年份  年份第一次出现的位置进行\r\n",
    "all_years = [df[day_date_list.index(str(year_list[x])):day_date_list.index(str(year_list[x]+1))] for x in range(len(year_list)-1)]\r\n",
    "# 最后一年\r\n",
    "all_years.append(df[day_date_list.index(str(year_list[-1])):])\r\n",
    "# 3.输出表\r\n",
    "[df_year.to_csv(\"./data/\"+str(year)+\"_info.csv\",index=False, sep=',')  for (year,df_year) in zip(year_list,all_years)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.地图视角"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 历史gps的热力图\r\n",
    "var heatmapData = [{\r\n",
    "\t\"lng\": 116.191031,\r\n",
    "\t\"lat\": 39.988585,\r\n",
    "\t\"count\": 10\r\n",
    "}, {\r\n",
    "\t\"lng\": 116.389275,\r\n",
    "\t\"lat\": 39.925818,\r\n",
    "\t\"count\": 11\r\n",
    "}]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1.制作生成形如上述的mapData.js文件，引入到html中即可\r\n",
    "# 2.读取 csv文件的经纬度信息\r\n",
    "import pandas as pd\r\n",
    "# res_path = './data/2017_info.csv'\r\n",
    "res_path = '../info.csv'\r\n",
    "df=pd.read_csv(res_path,header=0,sep=',') \r\n",
    "# 3.对经纬度进行 计数统计\r\n",
    "# df = df.tail(10)\r\n",
    "m = df.groupby(['longitude','latitude']).count()\r\n",
    "\r\n",
    "# 4.输出到js文件\r\n",
    "# \"lng\": 116.191031, \"lat\": 39.988585, \"count\": 10\r\n",
    "# 将 index 设置为列\r\n",
    "m = m.reset_index()\r\n",
    "df = pd.DataFrame(columns = ['lng','lat','count'])\r\n",
    "df[['lng','lat','count']] = m[['longitude','latitude','day']]\r\n",
    "heatmapData = \"var heatmapData_local = \" + df.to_json(orient = 'records')\r\n",
    "# print(heatmapData)\r\n",
    "# 打开一个文件\r\n",
    "fo = open(\"./data/heatmapData_local.js\", \"w\")\r\n",
    "fo.write(heatmapData)\r\n",
    "# 关闭打开的文件\r\n",
    "fo.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2把日记标注在地图上"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "# 1.制作生成形如上述的mapData.js文件，引入到html中即可\r\n",
    "# 2.读取 csv文件的经纬度信息\r\n",
    "def gen_diarymapData(filepath,desPath):\r\n",
    "    df=pd.read_csv(filepath,header=0,sep=',') \r\n",
    "    # 3.将需要在地图上显示信息 day title longitude latitude输出到js文件\r\n",
    "    d = pd.DataFrame(columns = ['lng','lat','day','title'])\r\n",
    "    d[['lng','lat','day','title']] = df[['longitude','latitude','day','title']]\r\n",
    "    # print(heatmapData)\r\n",
    "    # 打开一个文件\r\n",
    "    with open(desPath, 'w', encoding='utf-8') as file:\r\n",
    "        diarymapData = \"var diarymapData = \" + d.to_json(orient = 'records',force_ascii=False)\r\n",
    "        file.write(diarymapData)\r\n",
    "        # 关闭打开的文件\r\n",
    "        file.close()\r\n",
    "res_path = '../info.csv'\r\n",
    "desPath = './data/diarymapData.js'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3使用 leaflet timeline 进行滑动显示\r\n",
    "按格式进行输出到文件\r\n",
    "eqfeed_callback(\r\n",
    "    {\"type\":\"FeatureCollection\",\r\n",
    "        \"metadata\":{\r\n",
    "        \"generated\":1630507840000,\r\n",
    "        \"title\":\"USGS All Earthquakes, Past Day\",\"status\":200,\"api\":\"1.10.3\",\"count\":292},\r\n",
    "        \"features\":[\r\n",
    "                        {   \"type\":\"Feature\",\r\n",
    "                            \"properties\":{\r\n",
    "                                \"mag\":1,\r\n",
    "                                \"time\":1609430400000,\r\n",
    "                                \"title\":\"title1\"\r\n",
    "                                },\r\n",
    "                            \"geometry\":{\r\n",
    "                                \"type\":\"Point\",\r\n",
    "                                \"coordinates\":[-141.1344,60.2327]},\r\n",
    "                            // \"id\":\"ak021b7rquc6\"\r\n",
    "                            },\r\n",
    "                        }\r\n",
    "                    ],\r\n",
    "        });"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4.1 json格式的转换"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 例子1\r\n",
    "import json\r\n",
    "df = pd.DataFrame(\r\n",
    "    [['CompanyA',' Orange ' , 3000], \r\n",
    "    ['CompanyB',' Apple ' , 2000],\r\n",
    "    ['CompanyB', ' Grapes ' , 1000],\r\n",
    "    ],\r\n",
    "    columns=[\"company\",\"productName\",\"price\"],\r\n",
    ")\r\n",
    "m = df.groupby(['company'])['productName','price'].apply(lambda x: x.to_dict('r'))\r\n",
    "m.to_json(orient=\"columns\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  例2\r\n",
    "import json\r\n",
    "df = pd.DataFrame(\r\n",
    "    [['PnL', ' Company A',' Orange ' , 3000], \r\n",
    "    ['PnL', ' Company B',' Apple ' , 2000],\r\n",
    "    ['PnL', ' Company B', ' Grapes ' , 1000],\r\n",
    "    ['Tax', ' Company A',' Orange ' , 3000],\r\n",
    "    ['Tax', ' Company B',' Apple ' , 2000],\r\n",
    "    ['Tax', ' Company B',' Grapes ' , 1000]],\r\n",
    "    columns=[\"type\", \"company\",\"productName\",\"price\"],\r\n",
    ")\r\n",
    "d = (df.groupby(['type','company'])['productName','price']\r\n",
    "       .apply(lambda x: x.to_dict('r'))\r\n",
    "       .reset_index(name='data')\r\n",
    "       .groupby('type')['company','data']\r\n",
    "       .apply(lambda x: x.set_index('company')['data'].to_dict())\r\n",
    "       .to_json())\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\r\n",
    "import pandas as pd\r\n",
    "res_path = './data/2021_info.csv'\r\n",
    "# res_path = '../info.csv'\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1.制作生成形如上述的mapData.js文件，引入到html中即可\r\n",
    "# 2.读取 csv文件的经纬度信息\r\n",
    "def gen_traceData(res_path,desPath):\r\n",
    "    df=pd.read_csv(res_path,header=0,sep=',') \r\n",
    "    # df = df[:100][['day','title','longitude','latitude']]\r\n",
    "    df = df[['day','title','longitude','latitude']]\r\n",
    "    # 查找包含空值的行\r\n",
    "    # df[df.isnull().T.any()]\r\n",
    "    # 删除包含空值的行\r\n",
    "    df.dropna(axis=0, how='any', inplace=True)\r\n",
    "    # 3.将需要在地图上显示信息 day title longitude latitude输出到json文件\r\n",
    "    dicList = [] \r\n",
    "    time_list = [int(time.mktime(time.strptime(x, '%Y/%m/%d %H:%M'))*1000) for x in list(df.day)]\r\n",
    "    for gtime,title,lng,lat,day in zip(time_list,list(df['title']),list(df['longitude']),list(df['latitude']),list(df['day'])):\r\n",
    "        # print(time,title,lng,lat)\r\n",
    "        dicList.append({\r\n",
    "            \"type\":\"Feature\",\r\n",
    "            \"properties\":{  \r\n",
    "                            \"mag\":1,\r\n",
    "                            \"day\":day,\r\n",
    "                            \"time\":gtime,\r\n",
    "                            \"title\":title,\r\n",
    "                            },\r\n",
    "            \"geometry\":{\"type\":\"Point\",\r\n",
    "                            \"coordinates\":[lng,lat]},}\r\n",
    "        )\r\n",
    "    # 写进json文件\r\n",
    "    eqfeed_callback =  {\r\n",
    "            \"type\":\"FeatureCollection\",\r\n",
    "            \"metadata\":{\r\n",
    "            \"generated\":int(time.time()*1000),\r\n",
    "            \"title\":\"USGS All Earthquakes, Past Day\",\"status\":200,\"api\":\"1.10.3\",\"count\":len(df)},\r\n",
    "            \"features\":dicList}\r\n",
    "    \r\n",
    "    with open(desPath, 'w', encoding='utf-8') as file:\r\n",
    "        traceData = \"eqfeed_callback(\"+str(eqfeed_callback)+\");\"\r\n",
    "        file.write(traceData)\r\n",
    "        file.close()\r\n",
    "res_path = '../info.csv'\r\n",
    "desPath = './data/traceData.json'\r\n",
    "gen_traceData(res_path,desPath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.html to markdown"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from natsort import natsorted\r\n",
    "dir_name = \"../我的抗战2.0\"\r\n",
    "dirList = os.listdir(dir_name)\r\n",
    "htmlList = []\r\n",
    "for f in dirList:\r\n",
    "    if(f.endswith(\".html\")):\r\n",
    "        htmlList.append(f)\r\n",
    "# 按照数字开头进行排序 厉害\r\n",
    "htmlList = natsorted(htmlList)\r\n",
    "h_list = htmlList\r\n",
    "year_html_list = [htmlList[0:370],htmlList[370:725],htmlList[725:1071],htmlList[1071:]]\r\n",
    "h_lists = []\r\n",
    "years = ['2018','2019','2020','2021']\r\n",
    "for h_list,year in zip(year_html_list,years):\r\n",
    "    h_list = [dir_name + \"/\" + x for x in h_list]\r\n",
    "    h_lists.append(h_list)\r\n",
    "    descpath = dir_name + \"/\" + year +\".md\"\r\n",
    "    # htmls_to_md(h_list,descpath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "from markdownify import markdownify as md\r\n",
    "def htmls_to_md(srcpath,descpath):\r\n",
    "    res = \"\"\r\n",
    "    # t0 = \"\"\r\n",
    "    for path in srcpath:\r\n",
    "        # 读取html格式文件\r\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\r\n",
    "            htmlpage = f.read()\r\n",
    "        # 处理html格式文件中的内容\r\n",
    "            text = md(htmlpage)  \r\n",
    "            text = text.replace(\"body, td {\\n font-family: 微软雅黑;\\n font-size: 10pt;\\n }\\n\",\"\")\r\n",
    "            # 1.去除多余的换行\r\n",
    "            text = text.replace(\"\\n\\n\\n\\n\",\"\")\r\n",
    "            # 2.生成换行\r\n",
    "            text = text.replace(\"  \\n\",\"  \\n\\n\")\r\n",
    "            # 3.去掉特殊字符\r\n",
    "            text = text.replace(\"\\xa0\",\" \")\r\n",
    "            # 4.去掉标题  只去掉匹配的第一个\r\n",
    "            text = re.sub(r'\\n\\n.*\\n', \"\", text,1)\r\n",
    "            # 5.二次转化视频标签\r\n",
    "            text = gen_video_tag(text)\r\n",
    "            res +=text\r\n",
    "    with open(descpath, 'w', encoding='UTF-8') as f:\r\n",
    "        f.write(res)\r\n",
    "    # return res\r\n",
    "srcpath = [h_lists[2][10]]\r\n",
    "htmls_to_md(srcpath,\"../我的抗战2.0/t.md\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "# 二次转换视频\r\n",
    "# 原始  <a href=\"dirname/v_name.mp4\"> </a> \r\n",
    "# 经md转换后  [![1580400084168.mp4](4.11 8 h  声控游戏  刷线程进程基础研究语音控制  小黑屋午睡下不为例 二哥百果_files/07cc80e28ec6357bf86b517536b783e8.png)](4.11 8 h  声控游戏  刷线程进程基础研究语音控制  小黑屋午睡下不为例 二哥百果_files/1580400084168.mp4)\r\n",
    "# 二次转化  <video src=\"dirname/v_name.mp4\"></video>\r\n",
    "import re\r\n",
    "def gen_video_tag(s):\r\n",
    "    return re.sub('(?P<value>\\[.*?mp4\\))', video_match, s)\r\n",
    "\r\n",
    "def video_match(matched):\r\n",
    "    value = matched.group('value')\r\n",
    "    value = re.sub('(?P<value>\\[.*\\])', \"\\n\", value)\r\n",
    "    if value.endswith(\".mp4)\"):\r\n",
    "        value = value.replace(\"(\",\"<video src=\\\"\").replace(\")\",\"\\\"></video>\")\r\n",
    "    return value\r\n",
    "# s = \"<div><a href=\\\"4.11 8 h  声控游戏  刷线程进程基础研究语音控制  小黑屋午睡下不为例 二哥百果_files/1580400084168.mp4\\\"><img src=\\\"4.11 8 h  声控游戏  刷线程进程基础研究语音控制  小黑屋午睡下不为例 二哥百果_files/07cc80e28ec6357bf86b517536b783e8.png\\\" alt=\\\"1580400084168.mp4\\\"></a></div>\"\r\n",
    "# s = md(s)\r\n",
    "# s +=s\r\n",
    "# # 提取 形如此类的字符串 (...)\r\n",
    "# print(gen_video_tag(s))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# h_list = htmlList[:10]\r\n",
    "h_list = htmlList\r\n",
    "dir_name = \"../我的抗战1.0\"\r\n",
    "h_list = [dir_name + \"/\" + x for x in h_list]\r\n",
    "descpath = dir_name + \"/\" + \"kkk.md\"\r\n",
    "htmls_to_md(h_list,descpath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "h_list"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.6.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 32-bit"
  },
  "interpreter": {
   "hash": "ec7c6df8f2f99fa20fbf736f60336ad208c85ecac47a89374b2e832ec7eb482e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}