{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.从html提取信息写入csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 调用外部的py\r\n",
    "from demo import *\r\n",
    "hello()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.将空缺的日期补齐"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from datetime import datetime\r\n",
    "# First：找到没有进行创作的日子\r\n",
    "# 1.读取文件 第一行为列名称\r\n",
    "df=pd.read_csv('../csv绘图/info.csv',header=0,sep=',') #filename可以直接从盘符开始，标明每一级的文件夹直到csv文件，header=None表示头部为空，sep=' '表示数据间使用空格作为分隔符，如果分隔符是逗号，只需换成 ‘，’即可。\r\n",
    "# 2.按日期进行升序排列\r\n",
    "df2 = df.sort_values(by='day')\r\n",
    "# 3.提取出 day列  将其转化为 yyyy/mm/dd的格式  将 str=2017/1/27 23:06  转化为  datetime类型\r\n",
    "d1 = [datetime.strptime(x, '%Y/%m/%d %H:%M') for x in list(df2[\"day\"])]\r\n",
    "#       将 datetime 转化为 str 2017/01/27\r\n",
    "d_2017 = [datetime.strftime(x, '%Y/%m/%d') for x in list(d1)]\r\n",
    "# 4.创建全年的日子\r\n",
    "all_2017=[datetime.strftime(x,'%Y/%m/%d') for x in list(pd.date_range(start='2017/01/01',  end=\"2017/12/31\"))]\r\n",
    "# 5.将上两项做差求出没有记录的日子\r\n",
    "absent_2017 = [i for i in all_2017 if i not in d_2017]\r\n",
    "\r\n",
    "# 6.将没有记录的日子进行补充  [day:当天,title:空,longitude,latitude,num:0]\r\n",
    "dataframe = pd.DataFrame({'day': absent_2017,\r\n",
    "                            'title': [None]*len(absent_2017),\r\n",
    "                            'longitude': [None]*len(absent_2017),\r\n",
    "                            'latitude': [None]*len(absent_2017),\r\n",
    "                            'num': [0]*len(absent_2017)})\r\n",
    "dataframe.to_csv(\"./info.csv\",index=False, sep=',',mode='a', header=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "absent_2017 = ['2017/07/10','2017/07/12','2017/07/11']\r\n",
    "absent_2017_np = [time.mktime(time.strptime(x, '%Y/%m/%d')) for x in absent_2017]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\r\n",
    "import time       \r\n",
    "timeArray = time.strptime('2017/7/11 00:00', '%Y/%m/%d %H:%M')\r\n",
    "#转换成时间戳\r\n",
    "time.mktime(timeArray)     "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "absent_2017_np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.处理重复的日子"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 手动修改日期 并检查查看"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "# First：找到没有进行创作的日子\r\n",
    "# 1.读取文件 第一行为列名称\r\n",
    "res_path = './data/我的抗战2.0_2020_info.csv'\r\n",
    "# res_path = './data/2020_info.csv'\r\n",
    "df=pd.read_csv(res_path,header=0,sep=',') \r\n",
    "# 手动修改 日期  \r\n",
    "# 截取day的date————2017/02/02  是否唯一进行检查\r\n",
    "M = [x.split(\" \")[0] for x in df[\"day\"].tolist()]\r\n",
    "# 给原 df加一列 date\r\n",
    "df['day_date'] = M\r\n",
    "# 得到date重复的行  保留出现第一次的值  duplicated(keep=False)\r\n",
    "day_date = df[df['day_date'].duplicated(keep=False)]\r\n",
    "M = day_date[['day','title','day_date']]\r\n",
    "print(len(M))\r\n",
    "M"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2自动整理日期\r\n",
    "2019/08/19 12:00,,,,0,1566187200.0\r\n",
    "2019/08/20 15:02,3.196  转文全部被拒  心情失落  又开始考虑复原,0,0,28,1566284520.0\r\n",
    "2019/08/20 15:04,3.197  第一天便无法做到自律 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def edit_info(res_path):\r\n",
    "    # 手动修改 日期  \r\n",
    "    # 截取day的date————2017/02/02 12:26  -->  2017/02/02  是否唯一进行检查\r\n",
    "    df=pd.read_csv(res_path,header=0,sep=',') \r\n",
    "    M = [x.split(\" \")[0] for x in df[\"day\"].tolist()]\r\n",
    "    # 给原 df加一列 date\r\n",
    "    df['day_date'] = M\r\n",
    "    # 得到date重复的行  保留出现第一次的值  duplicated(keep=False)\r\n",
    "    day_date = df[df['day_date'].duplicated(keep=False)]\r\n",
    "    M = day_date[['day','title','day_date']]\r\n",
    "    print(\"重复的天数为：\",len(M))\r\n",
    "   \r\n",
    "    all_index = M.index.tolist() \r\n",
    "      # 剔除 0 元素\r\n",
    "    if 0 in all_index:\r\n",
    "        all_index.remove(0)\r\n",
    "    # 2.判断 index-1 是否为系统自动生成的 (num值是否为0)\r\n",
    "    del_index = []\r\n",
    "\r\n",
    "    for x in all_index:\r\n",
    "        if ((df[x:x+1].day_date == '2020/02/13').bool()):\r\n",
    "            print(\"2020/02/13\")\r\n",
    "        if ((df[x-1:x].num == 0).bool()):\r\n",
    "            print(\"删除行\",df[x-1:x].day)\r\n",
    "            del_index.append(x-1)\r\n",
    "            # df = df.drop([x-1])\r\n",
    "        # 3.是——删掉 index-1这一行 \r\n",
    "        # 4.修改 df[index] 的 date-1\r\n",
    "            temp = df[x:x+1]\r\n",
    "            dt = datetime.datetime.strptime(temp.at[temp.index.values[0],'day'],'%Y/%m/%d %H:%M')\r\n",
    "            out_date = (dt + datetime.timedelta(days=-1)).strftime('%Y/%m/%d %H:%M')\r\n",
    "            temp.day[x] = out_date\r\n",
    "    # 5.更新csv文件\r\n",
    "    df = df.drop(del_index)\r\n",
    "    df.to_csv(res_path,index=False, sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3将文件按年进行分表"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1.读取文件\r\n",
    "res_path = '../info.csv'\r\n",
    "df=pd.read_csv(res_path,header=0,sep=',') \r\n",
    "# 2.按照day_date(2018/01/01)  列的前四位  2018进行分表\r\n",
    "# day_date_list = list(df['day_date'])\r\n",
    "day_date_list = [x.split('/')[0] for x in list(df['day_date'])]\r\n",
    "year_list = list(set(day_date_list))\r\n",
    "year_list = [eval(x) for x in list(set(day_date_list))]\r\n",
    "# 3.将年份由小到大进行排序\r\n",
    "year_list.sort()\r\n",
    "# 4.分割年份  年份第一次出现的位置进行\r\n",
    "all_years = [df[day_date_list.index(str(year_list[x])):day_date_list.index(str(year_list[x]+1))] for x in range(len(year_list)-1)]\r\n",
    "# 最后一年\r\n",
    "all_years.append(df[day_date_list.index(str(year_list[-1])):])\r\n",
    "# 3.输出表\r\n",
    "[df_year.to_csv(\"./data/\"+str(year)+\"_info.csv\",index=False, sep=',')  for (year,df_year) in zip(year_list,all_years)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4将没有坐标的日子进行临近填充"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# info_path = 'D:\\MyJava\\mylifeImg\\others\\echart\\data\\demo.csv'\r\n",
    "# update_gps_info(info_path)\r\n",
    "dir = 'D:\\MyJava\\mylifeImg\\others\\echart\\data\\\\'\r\n",
    "update_gps_dir(dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 检验填充后的数据是否还有 gps 为 0 或 Nan\r\n",
    "import pandas as pd\r\n",
    "import time\r\n",
    "info_path = '../info.csv'\r\n",
    "m=pd.read_csv(info_path,header=0,sep=',') \r\n",
    "day_date = m[(m['longitude'].isna()) | (m['longitude']=='0')]['day_date']\r\n",
    "for x in day_date:\r\n",
    "    if time.strptime(x, '%Y/%m/%d')<time.localtime():\r\n",
    "        print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.地图视角"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 历史gps的热力图\r\n",
    "var heatmapData = [{\r\n",
    "\t\"lng\": 116.191031,\r\n",
    "\t\"lat\": 39.988585,\r\n",
    "\t\"count\": 10\r\n",
    "}, {\r\n",
    "\t\"lng\": 116.389275,\r\n",
    "\t\"lat\": 39.925818,\r\n",
    "\t\"count\": 11\r\n",
    "}]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1.制作生成形如上述的mapData.js文件，引入到html中即可\r\n",
    "# 2.读取 csv文件的经纬度信息\r\n",
    "import pandas as pd\r\n",
    "# res_path = './data/2017_info.csv'\r\n",
    "res_path = '../info.csv'\r\n",
    "df=pd.read_csv(res_path,header=0,sep=',') \r\n",
    "# 3.对经纬度进行 计数统计\r\n",
    "# df = df.tail(10)\r\n",
    "m = df.groupby(['longitude','latitude']).count()\r\n",
    "\r\n",
    "# 4.输出到js文件\r\n",
    "# \"lng\": 116.191031, \"lat\": 39.988585, \"count\": 10\r\n",
    "# 将 index 设置为列\r\n",
    "m = m.reset_index()\r\n",
    "df = pd.DataFrame(columns = ['lng','lat','count'])\r\n",
    "df[['lng','lat','count']] = m[['longitude','latitude','day']]\r\n",
    "heatmapData = \"var heatmapData_local = \" + df.to_json(orient = 'records')\r\n",
    "# print(heatmapData)\r\n",
    "# 打开一个文件\r\n",
    "fo = open(\"./data/heatmapData_local.js\", \"w\")\r\n",
    "fo.write(heatmapData)\r\n",
    "# 关闭打开的文件\r\n",
    "fo.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2把日记标注在地图上"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "# 1.制作生成形如上述的mapData.js文件，引入到html中即可\r\n",
    "# 2.读取 csv文件的经纬度信息\r\n",
    "def gen_diarymapData(filepath,desPath):\r\n",
    "    df=pd.read_csv(filepath,header=0,sep=',') \r\n",
    "    # 3.将需要在地图上显示信息 day title longitude latitude输出到js文件\r\n",
    "    d = pd.DataFrame(columns = ['lng','lat','day','title'])\r\n",
    "    d[['lng','lat','day','title']] = df[['longitude','latitude','day','title']]\r\n",
    "    # print(heatmapData)\r\n",
    "    # 打开一个文件\r\n",
    "    with open(desPath, 'w', encoding='utf-8') as file:\r\n",
    "        diarymapData = \"var diarymapData = \" + d.to_json(orient = 'records',force_ascii=False)\r\n",
    "        file.write(diarymapData)\r\n",
    "        # 关闭打开的文件\r\n",
    "        file.close()\r\n",
    "res_path = '../info.csv'\r\n",
    "desPath = './data/diarymapData.js'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 把日记 trace "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4使用 leaflet timeline 进行滑动显示\r\n",
    "按格式进行输出到文件\r\n",
    "eqfeed_callback(\r\n",
    "    {\"type\":\"FeatureCollection\",\r\n",
    "        \"metadata\":{\r\n",
    "        \"generated\":1630507840000,\r\n",
    "        \"title\":\"USGS All Earthquakes, Past Day\",\"status\":200,\"api\":\"1.10.3\",\"count\":292},\r\n",
    "        \"features\":[\r\n",
    "                        {   \"type\":\"Feature\",\r\n",
    "                            \"properties\":{\r\n",
    "                                \"mag\":1,\r\n",
    "                                \"time\":1609430400000,\r\n",
    "                                \"title\":\"title1\"\r\n",
    "                                },\r\n",
    "                            \"geometry\":{\r\n",
    "                                \"type\":\"Point\",\r\n",
    "                                \"coordinates\":[-141.1344,60.2327]},\r\n",
    "                            // \"id\":\"ak021b7rquc6\"\r\n",
    "                            },\r\n",
    "                        }\r\n",
    "                    ],\r\n",
    "        });"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4.1 json格式的转换"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 例子1\r\n",
    "import json\r\n",
    "df = pd.DataFrame(\r\n",
    "    [['CompanyA',' Orange ' , 3000], \r\n",
    "    ['CompanyB',' Apple ' , 2000],\r\n",
    "    ['CompanyB', ' Grapes ' , 1000],\r\n",
    "    ],\r\n",
    "    columns=[\"company\",\"productName\",\"price\"],\r\n",
    ")\r\n",
    "m = df.groupby(['company'])['productName','price'].apply(lambda x: x.to_dict('r'))\r\n",
    "m.to_json(orient=\"columns\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  例2\r\n",
    "import json\r\n",
    "df = pd.DataFrame(\r\n",
    "    [['PnL', ' Company A',' Orange ' , 3000], \r\n",
    "    ['PnL', ' Company B',' Apple ' , 2000],\r\n",
    "    ['PnL', ' Company B', ' Grapes ' , 1000],\r\n",
    "    ['Tax', ' Company A',' Orange ' , 3000],\r\n",
    "    ['Tax', ' Company B',' Apple ' , 2000],\r\n",
    "    ['Tax', ' Company B',' Grapes ' , 1000]],\r\n",
    "    columns=[\"type\", \"company\",\"productName\",\"price\"],\r\n",
    ")\r\n",
    "d = (df.groupby(['type','company'])['productName','price']\r\n",
    "       .apply(lambda x: x.to_dict('r'))\r\n",
    "       .reset_index(name='data')\r\n",
    "       .groupby('type')['company','data']\r\n",
    "       .apply(lambda x: x.set_index('company')['data'].to_dict())\r\n",
    "       .to_json())\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "source": [
    "import time\r\n",
    "import pandas as pd\r\n",
    "res_path = './data/2021_info.csv'\r\n",
    "# res_path = '../info.csv'\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "source": [
    "# 1.制作生成形如上述的mapData.js文件，引入到html中即可\r\n",
    "# 2.读取 csv文件的经纬度信息\r\n",
    "def gen_traceData(res_path,desPath):\r\n",
    "    df=pd.read_csv(res_path,header=0,sep=',') \r\n",
    "    # df = df[:100][['day','title','longitude','latitude']]\r\n",
    "    df = df[['day','title','longitude','latitude']]\r\n",
    "    # 查找包含空值的行\r\n",
    "    # df[df.isnull().T.any()]\r\n",
    "    # 删除包含空值的行\r\n",
    "    df.dropna(axis=0, how='any', inplace=True)\r\n",
    "    # 3.将需要在地图上显示信息 day title longitude latitude输出到json文件\r\n",
    "    dicList = [] \r\n",
    "    time_list = [int(time.mktime(time.strptime(x, '%Y/%m/%d %H:%M'))*1000) for x in list(df.day)]\r\n",
    "    for gtime,title,lng,lat,day in zip(time_list,list(df['title']),list(df['longitude']),list(df['latitude']),list(df['day'])):\r\n",
    "        # print(time,title,lng,lat)\r\n",
    "        dicList.append({\r\n",
    "            \"type\":\"Feature\",\r\n",
    "            \"properties\":{  \r\n",
    "                            \"mag\":1,\r\n",
    "                            \"day\":day,\r\n",
    "                            \"time\":gtime,\r\n",
    "                            \"title\":title,\r\n",
    "                            },\r\n",
    "            \"geometry\":{\"type\":\"Point\",\r\n",
    "                            \"coordinates\":[lng,lat]},}\r\n",
    "        )\r\n",
    "    # 写进json文件\r\n",
    "    eqfeed_callback =  {\r\n",
    "            \"type\":\"FeatureCollection\",\r\n",
    "            \"metadata\":{\r\n",
    "            \"generated\":int(time.time()*1000),\r\n",
    "            \"title\":\"USGS All Earthquakes, Past Day\",\"status\":200,\"api\":\"1.10.3\",\"count\":len(df)},\r\n",
    "            \"features\":dicList}\r\n",
    "    \r\n",
    "    with open(desPath, 'w', encoding='utf-8') as file:\r\n",
    "        traceData = \"eqfeed_callback(\"+str(eqfeed_callback)+\");\"\r\n",
    "        file.write(traceData)\r\n",
    "        file.close()\r\n",
    "res_path = './data/2021_info.csv'\r\n",
    "desPath = './data/traceData.json'\r\n",
    "gen_traceData(res_path,desPath)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.6.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 32-bit"
  },
  "interpreter": {
   "hash": "ec7c6df8f2f99fa20fbf736f60336ad208c85ecac47a89374b2e832ec7eb482e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}